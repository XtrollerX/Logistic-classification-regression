#The Mnist dataset was directly imported from pytorch itself


import torch
import torchvision
from torchvision.datasets import MNIST
import matplotlib as plt
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F

dataset = MNIST( root="data/", download =True)

#creating test dataset
test_dataset = MNIST(root="data/",train=False,transform=transforms.ToTensor())

#changing training data to tensor
dataset = MNIST(root="data/",train=True, transform=transforms.ToTensor())
img_tensor, label = dataset[0]
print(img_tensor.shape, label)



#creating of validation set
from torch.utils.data import random_split

train_ds, val_ds = random_split(dataset, [50000,10000])
print(len(train_ds),len(val_ds))

#data loading
from torch.utils.data import DataLoader

train_loader = DataLoader(train_ds,batch_size=128,shuffle=True)
val_loader = DataLoader(val_ds,batch_size=128)

def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):
    optimizer = opt_func(model.parameters(), lr)
    history = [] # for recording epoch-wise results
    
    for epoch in range(epochs):
        
        # Training Phase 
        for batch in train_loader:
            loss = model.training_step(batch)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
        
        # Validation phase
        result = evaluate(model, val_loader)
        model.epoch_end(epoch, result)
        history.append(result)

    return history
    
    
    
#creation of model
input_size= 28*28
num_classes = 10
class MnistModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(input_size, num_classes)
        
    def forward(self, xb):
        xb = xb.reshape(-1, 784)
        out = self.linear(xb)
        return out
    
    def training_step(self, batch):
        images, labels = batch 
        out = self(images)                  # Generate predictions
        loss = F.cross_entropy(out, labels) # Calculate loss
        return loss
    
    def validation_step(self, batch):
        images, labels = batch 
        out = self(images)                    # Generate predictions
        loss = F.cross_entropy(out, labels)   # Calculate loss
        acc = accuracy(out, labels)           # Calculate accuracy
        return {'val_loss': loss, 'val_acc': acc}
        
    def validation_epoch_end(self, outputs):
        batch_losses = [x['val_loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses
        batch_accs = [x['val_acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies
        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}
    
    def epoch_end(self, epoch, result):
        print("Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}".format(epoch, result['val_loss'], result['val_acc']))
    
model = MnistModel()
#Getting accuracy of results using the validation set, using the function defined above

result0 = evaluate(model, val_loader)
result0
#training our model, changing the weights,bias and gradient
history1 = fit(5, 0.001, model, train_loader, val_loader)    
